{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid approach phrase preprocessing\n",
    "\n",
    "In this tutorial, we will cover the phrase preprocessing step for the hybrid-dictionary-ner approach. An alternative (and compatible) more general preprocessing can be found in the medline-preprocessing repository in [this tutorial](https://github.com/zbmed-semtec/medline-preprocessing/blob/main/docs/Phrase_Preprocessing_Tutorial/tutorial_phrase_preprocessing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. Retrieve TSV files from TREC or RELISH data sets\n",
    "\n",
    "    - Use the recommended BioC-approach to generate .tsv files from the data sets.\n",
    "    \n",
    "    - Remove structure words, by using the Structure_Words_removal module in [this tutorial](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/docs/Structure_Words_removal).."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports\n",
    "\n",
    "First, we need to import the libraries from the code folder. To do so, change the `repository_path` variable to indicate the root path of the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "repository_path = os.path.expanduser(\"~/hybrid-dictionary-ner-doc2vec-doc-relevance\")\n",
    "\n",
    "sys.path.append(f\"{repository_path}/code/preprocessing/\")\n",
    "os.chdir(repository_path)\n",
    "\n",
    "import logging\n",
    "import preprocess as pp\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the data and preprocess\n",
    "\n",
    "After reading the input data, the preprocessing function is executed. The code process is described later in \"Code Strategy\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/preprocessing/RELISH/RELISH_documents_20220628_ann_swr.tsv\"\n",
    "#input_path = \"data/preprocessing/TREC/TREC_documents_20220628_ann_swr.tsv\"\n",
    "\n",
    "#input_path = \"../data_full/RELISH/RELISH_documents_20220628_ann_swr.tsv\"\n",
    "#input_path = \"../data_full/TREC/TREC_documents_20220628_ann_swr.tsv\"\n",
    "\n",
    "data = pp.read_data(input_path)\n",
    "data = pp.preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save the preprocess data\n",
    "\n",
    "The output is either stored in `.tsv` or `.npy` format. The `.tsv` is smaller in disk and faster to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/preprocessing/RELISH/RELISH_tokens.tsv\"\n",
    "#output_path = \"data/preprocessing/TREC/TREC_tokens.tsv\"\n",
    "\n",
    "#output_path = \"../data_full/RELISH/RELISH_tokens.tsv\"\n",
    "#output_path = \"../data_full/TREC/TREC_tokens.tsv\"\n",
    "\n",
    "pp.save_output(data, output_path, npy_format=False)\n",
    "#pp.save_output(data, output_path, npy_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code strategy\n",
    "\n",
    "1. The input file must be in `.tsv` format, containing three columns: \"PMID\", \"title\", \"abstract\". The file is recommended to be pruned of structure words following [this tutorial](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/docs/Structure_Words_removal).\n",
    "\n",
    "2. The code loops through every row of the input `.tsv` and preprocess the title and abstract separately. \n",
    "\n",
    "3. The preprocess consists in:\n",
    "    \n",
    "    * (Optional) If the function parameter `process_abv` is set to `True` (`False` by default), an experimental abbreviation algorithm is executed to find and combine terms like \"E. coli\" or \"S. aureus\" into a single word like \"e.coli\". This process is not well tested and not recommended unless necessary.\n",
    "\n",
    "    * Lowercase everything. \n",
    "\n",
    "    * Tokenize space-separated words. The text is split by white spaces and only alphanumeric characters and allowed punctuation is kept.\n",
    "\n",
    "    * Removes all special character except for the hyphens `-` (this can be modified using the function parameter `allowed_punctuation`).\n",
    "\n",
    "    * (Optional) Saves the results as a three-dimensional numpy array and saves it as a `.npy` file.\n",
    "\n",
    "    * (Optional) Saves the results as a three column `.tsv` file containing \"PMID\", \"title\" and \"abstract\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions\n",
    "\n",
    "* Instead of using the default phrase preprocessing found in medline-preprocessing ([here](https://github.com/zbmed-semtec/medline-preprocessing/blob/main/docs/Phrase_Preprocessing_Tutorial/tutorial_phrase_preprocessing.ipynb)), the steps produced in here are particular for the hybrid-dictionary-ner approach. The results produced are expectd to be the same, but execution time is greatly improved in this approach. The main difference is to not include the biological tokenizer `en_core_sci_lg` from the sciSpacy module, since its use is not recommended in this approach.\n",
    "\n",
    "* For the decisions related to the actual preprocess steps followed, please refer to the main documentation in [here](https://github.com/zbmed-semtec/medline-preprocessing#cleaning-for-word-embedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "The time to preprocess each dataset (TREC or RELISH) using 1 core of an Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz with 16GB of RAM running Ubuntu 20.04 LTS and Python 3.8.10 is:\n",
    "\n",
    "* RELISH (163189 publications): 2min 31s ± 538 ms on average.\n",
    "\n",
    "* TREC (32604 publications): 26.9 s ± 203 ms on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMOVE THIS LINE BEFORE FINAL VERSION COMMIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook docs/preprocessing/tutorial_preprocessing.ipynb to markdown\n",
      "[NbConvertApp] Writing 5019 bytes to docs/preprocessing/README.md\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert docs/preprocessing/tutorial_preprocessing.ipynb --to markdown --output README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
