{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid approach phrase preprocessing\n",
    "\n",
    "In this tutorial, we will cover the phrase preprocessing step for the hybrid-dictionary-ner approach. An alternative (and compatible) more general preprocessing can be found in the medline-preprocessing repository in [this tutorial](https://github.com/zbmed-semtec/medline-preprocessing/blob/main/docs/Phrase_Preprocessing_Tutorial/tutorial_phrase_preprocessing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. Remove structure words from the dataset by using the Structure_Words_removal module in [this tutorial](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/docs/Structure_Words_removal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports\n",
    "\n",
    "First, we need to import the libraries from the code folder. To do so, change the `repository_path` variable to indicate the root path of the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "repository_path = os.path.expanduser(\"~/hybrid-dictionary-ner-doc2vec-doc-relevance\")\n",
    "\n",
    "sys.path.append(f\"{repository_path}/code/preprocessing/\")\n",
    "os.chdir(repository_path)\n",
    "\n",
    "import logging\n",
    "import preprocess as pp\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the data and preprocess\n",
    "\n",
    "After reading the input data, the preprocessing function is executed. The code process is described later in \"Code Strategy\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/RELISH/RELISH_documents_20220628_ann_swr.tsv\"\n",
    "#input_path = \"data/TREC/TREC_documents_20220628_ann_swr.tsv\"\n",
    "\n",
    "data = pp.read_data(input_path)\n",
    "data = pp.preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save the preprocess data\n",
    "\n",
    "The output is either stored in TSV or NPY format. The TSV is usually smaller in disk and faster to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/RELISH/RELISH_tokens.tsv\"\n",
    "#output_path = \"data/TREC/TREC_tokens.tsv\"\n",
    "\n",
    "pp.save_output(data, output_path, npy_format=False)\n",
    "#pp.save_output(data, output_path, npy_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `preprocess.py` as a script\n",
    "\n",
    "The code file itself is prepared to work on their own given some parameters. In order to execute the script, run the following command:\n",
    "\n",
    "```bash\n",
    "preprocess.py [-h] -i INPUT [-o OUTPUT]\n",
    "```\n",
    "\n",
    "You must pass the following argument:\n",
    "\n",
    "* -i / --input: path to the TSV file to be preprocessed.\n",
    "\n",
    "Additionally, other parameters can be specified:\n",
    "\n",
    "* -o / --output: path to the output TSV file after preprocessing.\n",
    "\n",
    "* --npy_format: whether to save the output in NPY format. By default False\n",
    "\n",
    "If no output is provided, the text `_preprocessed` will be added to the input file and saved in the same location. If the output path specified is a TSV (NPY) format, but the `npy_format` argument is set to True (False), the file extension will take priority and the output will be stored in TSV (NPY). Note that by using the script directly, you cannot use the abbreviation method.\n",
    "\n",
    "An example of the command that will preprocess the example dataset in the data folder is:\n",
    "\n",
    "```bash\n",
    "python code/preprocessing/preprocess.py --input data/RELISH/RELISH_documents_20220628_ann_swr.tsv --output data/RELISH/RELISH_tokens.tsv\n",
    "\n",
    "python code/preprocessing/preprocess.py --input data/TREC/TREC_documents_20220628_ann_swr.tsv --output data/TREC/TREC_tokens.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Example of preprocessing:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Abstract Input</th>\n",
    "<th>Abstract Output</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "Despite the high MeSHQ000453 of MeSHD010300, the MeSHQ000503 of its gastrointestinal MeSHQ000175 remains poorly understood. to evaluate MeSHD003679 and defecatory MeSHQ000502 in MeSHD010361 with MeSHD010300 and age- and MeSHD012723-matched MeSHQ000517 and to correlate objective MeSHQ000175 with subjective MeSHQ000175.\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "despite the high meshq000453 of meshd010300 the meshq000503 of its gastrointestinal meshq000175 remains poorly understood to evaluate meshd003679 and defecatory meshq000502 in meshd010361 with meshd010300 and age- and meshd012723-matched meshq000517 and to correlate objective meshq000175 with subjective meshq000175\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code strategy\n",
    "\n",
    "1. The input file must be in TSV format, containing three columns: \"PMID\", \"title\", \"abstract\". The file is recommended to be pruned of structure words following [this tutorial](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/docs/Structure_Words_removal).\n",
    "\n",
    "2. The code loops through every row of the input TSV and preprocess the title and abstract separately. \n",
    "\n",
    "3. The preprocess consists in:\n",
    "    \n",
    "    * (Optional) If the function parameter `process_abv` is set to `True` (`False` by default), an experimental abbreviation algorithm is executed to find and combine terms like \"E. coli\" or \"S. aureus\" into a single word like \"e.coli\". This process is not well tested and not recommended unless necessary.\n",
    "\n",
    "    * Lowercase everything. \n",
    "\n",
    "    * Tokenize space-separated words. The text is split by white spaces and only alphanumeric characters and allowed punctuation is kept.\n",
    "\n",
    "    * Removes all special character except for the hyphens `-` (this can be modified using the function parameter `allowed_punctuation`).\n",
    "\n",
    "    * (Optional) Saves the results as a three-dimensional numpy array and saves it as a NPY file.\n",
    "\n",
    "    * (Optional) Saves the results as a three column TSV file containing \"PMID\", \"title\" and \"abstract\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions\n",
    "\n",
    "* Instead of using the default phrase preprocessing found in medline-preprocessing ([here](https://github.com/zbmed-semtec/medline-preprocessing/blob/main/docs/Phrase_Preprocessing_Tutorial/tutorial_phrase_preprocessing.ipynb)), the steps produced in here are particular for the hybrid-dictionary-ner approach. The results produced are expected to be the same, but execution time is greatly improved in this approach. The main difference is to not include the biological tokenizer `en_core_sci_lg` from the sciSpacy module, since its use is not recommended in this approach.\n",
    "\n",
    "* For the decisions related to the actual preprocess steps followed, please refer to the main documentation in [here](https://github.com/zbmed-semtec/medline-preprocessing#cleaning-for-word-embedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "The time to preprocess each dataset (TREC or RELISH) using 1 core of an Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz with 16GB of RAM running Ubuntu 20.04 LTS and Python 3.8.10 is:\n",
    "\n",
    "* RELISH (163189 publications): 2min 31s ± 538 ms on average.\n",
    "\n",
    "* TREC (32604 publications): 26.9 s ± 203 ms on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook docs/preprocessing/tutorial_preprocessing.ipynb to markdown\n",
      "[NbConvertApp] Writing 6597 bytes to docs/preprocessing/README.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert docs/preprocessing/tutorial_preprocessing.ipynb --to markdown --output README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
