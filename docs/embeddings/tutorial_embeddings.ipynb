{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Model and embeddings generation\n",
    "\n",
    "In this tutorial, we will cover the generation of the Doc2Vec model for the hybrid-dictionary-ner approach. The aim is to produce embeddings for each RELISH and TREC publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. Preprocessed tokens in NPY format or TSV format. They can be generated following the [preprocessing tutorial](https://github.com/zbmed-semtec/hybrid-dictionary-ner-doc2vec-doc-relevance/tree/main/docs/preprocessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model generation\n",
    "\n",
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Imports\n",
    "\n",
    "First, we need to import the libraries from the code folder. To do so, change the `repository_path` variable to indicate the root path of the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "repository_path = os.path.expanduser(\"~/hybrid-dictionary-ner-doc2vec-doc-relevance\")\n",
    "\n",
    "sys.path.append(f\"{repository_path}/code/embeddings/\")\n",
    "os.chdir(repository_path)\n",
    "\n",
    "import logging\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import create_model as cm\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Loading the data\n",
    "\n",
    "Next, we need to import the preprocessed tokens. A small sample is provided in the data folder. The `load_tokens()` function returns the title and abstract combined in one document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_path = \"data/RELISH/RELISH_tokens.tsv\"\n",
    "#tokens_path = \"data/TREC/TREC_tokens.tsv\"\n",
    "\n",
    "pmid, join_text = cm.load_tokens(tokens_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the `TaggedDocuments` required by `Doc2Vec` to generate and train the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = cm.generate_TaggedDocument(pmid, join_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Creating the model\n",
    "\n",
    "First, we need to choose the hyperparameters of the model. In this tutorial, we will only consider one combination of hyperparameters (for the hyperparameter optimization, please refer to the [tendency analysis](https://github.com/zbmed-semtec/hybrid-dictionary-ner-doc2vec-doc-relevance/blob/main/docs/tendency_analysis/tutorial_tendency_analysis.ipynb) tutorial). The easiest way to indicate the hyperparameters is to create a dictionary with the [available options](https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_d2v = {\n",
    "    \"vector_size\": 200, \n",
    "    \"window\": 5, \n",
    "    \"min_count\": 5, \n",
    "    \"epochs\": 5, \n",
    "    \"workers\": 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the model, use the `generate_doc2vec_model()` function with the tagged data and the model parameters. This function automatically creates the required vocabulary for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cm.generate_doc2vec_model(tagged_data, params_d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training the model\n",
    "\n",
    "The function `train_doc2vec_model` is responsible for training the previously generated model. The argument verbose determines the information to receive from the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-11 10:24:02,639 \tEpoch #0 start\n",
      "2022-09-11 10:24:02,669 \tEpoch #0 end\n",
      "2022-09-11 10:24:02,670 \tEpoch #1 start\n",
      "2022-09-11 10:24:02,699 \tEpoch #1 end\n",
      "2022-09-11 10:24:02,699 \tEpoch #2 start\n",
      "2022-09-11 10:24:02,726 \tEpoch #2 end\n",
      "2022-09-11 10:24:02,727 \tEpoch #3 start\n",
      "2022-09-11 10:24:02,755 \tEpoch #3 end\n",
      "2022-09-11 10:24:02,756 \tEpoch #4 start\n",
      "2022-09-11 10:24:02,780 \tEpoch #4 end\n",
      "2022-09-11 10:24:02,781 --- Time to train: 0.14 seconds\n"
     ]
    }
   ],
   "source": [
    "cm.train_doc2vec_model(model, tagged_data, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be stored to later be used by `save_doc2vec_model()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = \"data/RELISH/RELISH_hybrid.model\"\n",
    "#output_model_path = \"data/TREC/TREC_hybrid.model\"\n",
    "\n",
    "cm.save_doc2vec_model(model, output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Store the embeddings\n",
    "\n",
    "The embeddings can be stored either in the model itself or as a separate entity outside Doc2Vec (this allows to calculate cosine similarity without the need of Doc2Vec once the embeddings are already generated).\n",
    "\n",
    "The embeddings are stored into a dataframe with two columns: pmids and embeddings. The output is stored in pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/RELISH/RELISH_embeddings.pkl\"\n",
    "#output_path = \"data/TREC/TREC_embeddings.pkl\"\n",
    "\n",
    "cm.save_doc2vec_embeddings(model, pmid, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `create_model.py` as a script\n",
    "\n",
    "The code file itself is prepared to work on their own given some parameters. In order to execute the script, run the following command:\n",
    "\n",
    "```bash\n",
    "create_model.py [-h] -i INPUT [-o OUTPUT] [--embeddings EMBEDDINGS]\n",
    "```\n",
    "\n",
    "You must pass the following argument:\n",
    "\n",
    "* -i / --input: path to the TSV file with the tokens.\n",
    "\n",
    "Additionally, other parameters can be specified:\n",
    "\n",
    "* -o / --output: path to the output model.\n",
    "\n",
    "* --embeddings: path to the output embeddings.\n",
    "\n",
    "An example of the command that will generate the model from the tokens in the data folder is:\n",
    "\n",
    "```bash\n",
    "python code/embeddings/create_model.py --input data/RELISH/RELISH_tokens.tsv --embeddings data/RELISH/RELISH_embeddings.pkl --output data/RELISH/RELISH_hybrid.model\n",
    "\n",
    "python code/embeddings/create_model.py --input data/TREC/TREC_tokens.tsv --embeddings data/TREC/TREC_embeddings.pkl --output data/TREC/TREC_hybrid.model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "An example of the output PKL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17977838</td>\n",
       "      <td>[0.03675682470202446, -0.06266345828771591, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17997202</td>\n",
       "      <td>[0.04471859708428383, -0.08651144802570343, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18207447</td>\n",
       "      <td>[0.0328378789126873, -0.04986971616744995, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18235058</td>\n",
       "      <td>[0.005275525618344545, -0.0077206785790622234,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18251855</td>\n",
       "      <td>[0.0480794794857502, -0.07831325381994247, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>29394417</td>\n",
       "      <td>[0.035523608326911926, -0.06166147068142891, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>29483077</td>\n",
       "      <td>[0.04499121755361557, -0.08086196333169937, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>29655810</td>\n",
       "      <td>[0.0457320399582386, -0.08665464073419571, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>29721798</td>\n",
       "      <td>[0.06336772441864014, -0.09875210374593735, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>29797481</td>\n",
       "      <td>[0.040957752615213394, -0.06522607058286667, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pmids                                         embeddings\n",
       "0    17977838  [0.03675682470202446, -0.06266345828771591, -0...\n",
       "1    17997202  [0.04471859708428383, -0.08651144802570343, -0...\n",
       "2    18207447  [0.0328378789126873, -0.04986971616744995, -0....\n",
       "3    18235058  [0.005275525618344545, -0.0077206785790622234,...\n",
       "4    18251855  [0.0480794794857502, -0.07831325381994247, -0....\n",
       "..        ...                                                ...\n",
       "145  29394417  [0.035523608326911926, -0.06166147068142891, -...\n",
       "146  29483077  [0.04499121755361557, -0.08086196333169937, -0...\n",
       "147  29655810  [0.0457320399582386, -0.08665464073419571, -0....\n",
       "148  29721798  [0.06336772441864014, -0.09875210374593735, -0...\n",
       "149  29797481  [0.040957752615213394, -0.06522607058286667, -...\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_pickle(\"data/RELISH/RELISH_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling of the relevance matrix\n",
    "\n",
    "Additionally, this tutorial will also explain how to fill a relevance matrix with the Cosine Similarities calculated using the Doc2vec model. It is recommended to use the [more general approach](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/code/Cosine_Similarity) to calculate the similarities directly from the embeddings and not from the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "### Step 1: Load the libraries\n",
    "\n",
    "First, we load the required library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "repository_path = os.path.expanduser(\"~/hybrid-dictionary-ner-doc2vec-doc-relevance\")\n",
    "\n",
    "sys.path.append(f\"{repository_path}/code/embeddings/\")\n",
    "os.chdir(repository_path)\n",
    "\n",
    "import logging\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import fill_relevance_matrix as frm\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load relevance matrix and model\n",
    "\n",
    "Set the directory path of the relevance matrix and of the model. If you are running this code with the above tutorial, there is no need to load the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rm = \"data/RELISH/RELISH_relevance_matrix.tsv\"\n",
    "input_model = \"data/RELISH/RELISH_hybrid.model\"\n",
    "\n",
    "#input_rm = \"data/TREC/TREC_relevance_matrix.tsv\"\n",
    "#input_model = \"data/TREC/TREC_hybrid.model\"\n",
    "\n",
    "relevance_matrix = frm.load_relevance_matrix(input_rm)\n",
    "model = frm.load_d2v_model(input_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fill the relevance matrix and save it\n",
    "\n",
    "In this step, it is recommended to use the multiprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-11 10:24:04,160 --- Time to fill: 0.30 seconds\n"
     ]
    }
   ],
   "source": [
    "#filled_relevance_matrix = frm.fill_relevance_matrix(relevance_matrix, model, verbose=1)\n",
    "filled_relevance_matrix = frm.fill_relevance_matrix_multiprocess(relevance_matrix, model, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the number of cores with the `num_processess` parameter. By default, it is set to the number of cores of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save the filled relevance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/RELISH/RELISH_filled_relevance_matrix.tsv\"\n",
    "#output_path = \"data/TREC/TREC_filled_relevance_matrix.tsv\"\n",
    "\n",
    "frm.save_rel_matrix(filled_relevance_matrix, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `fill_relevance_matrix.py` as a script\n",
    "\n",
    "The code file itself is prepared to work on their own given some parameters. In order to execute the script, run the following command:\n",
    "\n",
    "```bash\n",
    "fill_relevance_matrix.py [-h] --input_rm INPUT_RM --input_model INPUT_MODEL --output OUTPUT [--verbose VERBOSE] [--multithread {0,1}] [--num_cores NUM_CORES]\n",
    "```\n",
    "\n",
    "You must pass the following argument:\n",
    "\n",
    "* --input_rm: path to the TSV file with the tokens.\n",
    "\n",
    "* --input_model: input path to the Doc2Vec model.\n",
    "\n",
    "* --output: output path to the filled relevance matrix.\n",
    "\n",
    "Additionally, other parameters can be specified:\n",
    "\n",
    "* --verbose: the level of information logged in the process.\n",
    "\n",
    "* --multithread: whether to use multiprocessing or not. It is recommended to set to 1. Optionally, set the number of cores to be used with 'num_cores' argument.\n",
    "\n",
    "* --num_cores: number of cores to use if multiprocessing is available. By default, leave to 'None' to use all cores.\n",
    "\n",
    "An example of the command that will fill the relevance matrix from the data folder is:\n",
    "\n",
    "```bash\n",
    "python code/embeddings/fill_relevance_matrix.py --input_rm data/RELISH/RELISH_relevance_matrix.tsv --input_model data/RELISH/RELISH_hybrid.model --output data/RELISH/RELISH_filled_relevance_matrix.tsv --verbose 1\n",
    "\n",
    "python code/embeddings/fill_relevance_matrix.py --input_rm data/TREC/TREC_relevance_matrix.tsv --input_model data/TREC/TREC_hybrid.model --output data/TREC/TREC_filled_relevance_matrix.tsv --verbose 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code strategy\n",
    "\n",
    "1. The pipeline accepts either a TSV or a NPY format as the input tokens. Usually, TSV format is preferred since its size on disk is smaller. The tokens should have been generated following the [preprocessing tutorial](https://github.com/zbmed-semtec/hybrid-dictionary-ner-doc2vec-doc-relevance/blob/main/docs/preprocessing/tutorial_preprocessing.ipynb).\n",
    "    \n",
    "    If using a custom TSV file, three columns are required: \"PMID\", \"title\" and \"abstract\".\n",
    "    \n",
    "\n",
    "2. The input of Doc2Vec models should be a list of `TaggedDocument`. In this case, we join the title and the abstract as a single paragraph and set the PMID as the tag. \n",
    "\n",
    "3. To decide on the hyperparameters, we performed a literature review of common Doc2Vec hyperparameters and their different possibilities. Results can be consulted [here](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/resources). We opted to use a dictionary of the hyperparameters, since this allows for an easy hyperparameter search implementation. Please, refer to the [distribution analysis](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/docs/Distribution_Analysis/hp_optimization) study in which the different hyperparameters are tested.\n",
    "\n",
    "4. To create the model, we just need the input hyperparameters and the tagged data to build a vocabulary. The vocabulary building step is executed in here in order to better separate the model creating from its training, but it could be constructed at the training time without any problem.\n",
    "\n",
    "5. To train the model, we use the number of epochs selected in the model parameters and the examples provided in the tagged data. Additionally, we provide a logging implementation to obtain information about the training:\n",
    "\n",
    "    * Warning/Errors (verbose = 0): default information logged by `gensim` if any error occurs during the training.\n",
    "\n",
    "    * Info (verbose = 1): provides information about the total training time (in seconds).\n",
    "\n",
    "    * Debug (verbose = 2): shows the time at which every epoch starts and finishes.\n",
    "\n",
    "    Lastly, once the model is trained, it can be stored in disk to load later.\n",
    "\n",
    "6. The next step is to generate the embeddings for each publication. This allows to later calculate the cosine similarities for each pair of documents without the need of the `Doc2Vec` model. \n",
    "\n",
    "7. If we wanted to use the model to fill a relevance matrix, the additional file `fill_relevance_matrix.py` reads the given relevance matrix, applies a verification process to check if the provided file matches the requirements and then compares every PMID in the two columns while filling a fourth column containing the cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions\n",
    "\n",
    "* The parameters are passed to the `generate_doc2vec_model()` function as a dictionary to later facilitate the inclusion of hyperparameter optimization, as well as providing an easy to use and implement feature.\n",
    "\n",
    "* In the training process, we employed the `logging` library to provide information about training to the end user. The information reported is selected with the `verbose` parameter as explained in the section before.\n",
    "\n",
    "* The embeddings are stored into a `pandas` `DataFrame` with two columns: pmids and embeddings. The embeddings' column stores lists of the embeddings for that publication. We decided to store the dataframe in pickle format for its advantages ([resources](https://github.com/zbmed-semtec/bert-embeddings-doc-relevance/blob/main/playground/speed_size_comparision.ipynb)).\n",
    "\n",
    "* We decided to provide an additional method of filling the relevance matrix (other than [the more general approach](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/code/Cosine_Similarity)) in case the user prefers to use the model directly and not the intermediary pickle format embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "The time to train each dataset (TREC or RELISH) using 8 cores of an Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz with 16GB of RAM running Ubuntu 20.04 LTS and Python 3.8.10 is:\n",
    "\n",
    "* RELISH (163189 publications): 25 seconds per epoch on average.\n",
    "\n",
    "* TREC (32604 publications): 5 seconds per epoch on average.\n",
    "\n",
    "The time to fill each dataset using 16 cores of the same processor with the same configuration is:\n",
    "\n",
    "* RELISH (196680 comparisons): 35 seconds on average.\n",
    "\n",
    "* TREC (18466607 comparisons): 2m 15 seconds on average.\n",
    "\n",
    "These results will greatly depend on the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook docs/embeddings/tutorial_embeddings.ipynb to markdown\n",
      "[NbConvertApp] Writing 15399 bytes to docs/embeddings/README.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert docs/embeddings/tutorial_embeddings.ipynb --to markdown --output README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
