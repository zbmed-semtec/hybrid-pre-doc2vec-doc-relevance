{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Model and embeddings generation\n",
    "\n",
    "In this tutorial, we will cover the generation of the Doc2Vec model for the hybrid-dictionary-ner approach. The aim is to produce embeddings for each RELISH and TREC publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. Preprocessed tokens in .npy format or .tsv format. They can be generated following the [preprocessing tutorial](https://github.com/zbmed-semtec/hybrid-dictionary-ner-doc2vec-doc-relevance/blob/main/docs/preprocessing/tutorial_preprocessing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports\n",
    "\n",
    "First, we need to import the libraries from the code folder. To do so, change the `repository_path` variable to indicate the root path of the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "repository_path = os.path.expanduser(\"~/hybrid-dictionary-ner-doc2vec-doc-relevance\")\n",
    "\n",
    "sys.path.append(f\"{repository_path}/code/embeddings/\")\n",
    "os.chdir(repository_path)\n",
    "\n",
    "import logging\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import create_model as cm\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading the data\n",
    "\n",
    "Next, we need to import the preprocessed tokens. An small sample is provided in the data folder. The `load_tokens()` function returns the title and abstract combined in one document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_path = \"data/embeddings/RELISH/RELISH_tokens.tsv\"\n",
    "#tokens_path = \"data/embeddings/TREC/TREC_tokens.tsv\"\n",
    "\n",
    "#tokens_path = \"../data_full/RELISH/RELISH_tokens.tsv\"\n",
    "#tokens_path = \"../data_full/TREC/TREC_tokens.tsv\"\n",
    "\n",
    "pmid, join_text = cm.load_tokens(tokens_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the `TaggedDocuments` required by `Doc2Vec` to generate and train the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = cm.generate_TaggedDocument(pmid, join_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating the model\n",
    "\n",
    "First, we need to choose the hyperparameters of the model. In this tutorial, we will only consider one combination of hyperparameters (for the hyperparameter optimization, please refer to the [tendency analysis](https://github.com/zbmed-semtec/hybrid-dictionary-ner-doc2vec-doc-relevance/blob/main/docs/tendency_analysis/tutorial_tendency_analysis.ipynb) tutorial). The easiest way to indicate the hyperparameters is to create a dictionary with the [available options](https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_d2v = {\n",
    "    \"dm\": 0,\n",
    "    \"vector_size\": 256, \n",
    "    \"window\": 7, \n",
    "    \"min_count\": 5, \n",
    "    \"epochs\": 10, \n",
    "    \"workers\": 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the model, use the `generate_doc2vec_model()` function with the tagged data and the model parameters. This function automatically creates the required vocabulary for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cm.generate_doc2vec_model(tagged_data, params_d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training the model\n",
    "\n",
    "The function `train_doc2vec_model` is responsible for training the previously generated model. The argument verbose determines the information to receive from the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 15:52:16,233 \tEpoch #0 start\n",
      "2022-08-02 15:52:16,268 \tEpoch #0 end\n",
      "2022-08-02 15:52:16,269 \tEpoch #1 start\n",
      "2022-08-02 15:52:16,292 \tEpoch #1 end\n",
      "2022-08-02 15:52:16,292 \tEpoch #2 start\n",
      "2022-08-02 15:52:16,322 \tEpoch #2 end\n",
      "2022-08-02 15:52:16,322 \tEpoch #3 start\n",
      "2022-08-02 15:52:16,347 \tEpoch #3 end\n",
      "2022-08-02 15:52:16,348 \tEpoch #4 start\n",
      "2022-08-02 15:52:16,373 \tEpoch #4 end\n",
      "2022-08-02 15:52:16,373 \tEpoch #5 start\n",
      "2022-08-02 15:52:16,397 \tEpoch #5 end\n",
      "2022-08-02 15:52:16,398 \tEpoch #6 start\n",
      "2022-08-02 15:52:16,423 \tEpoch #6 end\n",
      "2022-08-02 15:52:16,424 \tEpoch #7 start\n",
      "2022-08-02 15:52:16,453 \tEpoch #7 end\n",
      "2022-08-02 15:52:16,454 \tEpoch #8 start\n",
      "2022-08-02 15:52:16,482 \tEpoch #8 end\n",
      "2022-08-02 15:52:16,482 \tEpoch #9 start\n",
      "2022-08-02 15:52:16,518 \tEpoch #9 end\n",
      "2022-08-02 15:52:16,519 --- Time to train: 0.29 seconds\n"
     ]
    }
   ],
   "source": [
    "cm.train_doc2vec_model(model, tagged_data, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be stored to later be used by `save_doc2vec_model()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = \"data/embeddings/RELISH/RELISH_hybrid_d2v.model\"\n",
    "#output_model_path = \"data/embeddings/TREC/TREC_hybrid_d2v.model\"\n",
    "\n",
    "#output_model_path = \"../data_full/RELISH/RELISH_hybrid_d2v.model\"\n",
    "#output_model_path = \"../data_full/TREC/TREC_hybrid_d2v.model\"\n",
    "\n",
    "cm.save_doc2vec_model(model, output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Store the embeddings\n",
    "\n",
    "The embeddings can be stored either in the model itself (recommended) or as a separate entity outside of Doc2Vec (this allows to calculate cosine similarity without the need of Doc2Vec once the embeddings are already generated).\n",
    "\n",
    "At the same time, the user can choose to store the embeddings into a single file or into multiple files using the same `save_doc2vec_embedding()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.save_doc2vec_embeddings(model, pmid[:15], \"data/embeddings/RELISH/DocumentVectors\", one_file = False)\n",
    "#cm.save_doc2vec_embeddings(model, pmid, \"data/embeddings/RELISH/RELISH_document_embeddings.npy\", one_file = True)\n",
    "\n",
    "#cm.save_doc2vec_embeddings(model, pmid[:15], \"data/embeddings/TREC/DocumentVectors\", one_file = False)\n",
    "#cm.save_doc2vec_embeddings(model, pmid, \"data/embeddings/TREC/TREC_document_embeddings.npy\", one_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code strategy\n",
    "\n",
    "1. The pipeline accepts either a `.tsv` or a `.npy` format as the input tokens. Usually, `.tsv` format is prefered since its size on disk is smaller. The tokens should have been generated following the [preprocessing tutorial](https://github.com/zbmed-semtec/hybrid-dictionary-ner-doc2vec-doc-relevance/blob/main/docs/preprocessing/tutorial_preprocessing.ipynb).\n",
    "    \n",
    "    If using a custom `.tsv` file, three columns are required: \"PMID\", \"title\" and \"abstract\".\n",
    "    \n",
    "\n",
    "2. The input of Doc2Vec models should be a list of `TaggedDocument`. In this case, we join the title and the abstract as a single paragraph and set the PMID as the tag. \n",
    "\n",
    "3. To decide on the hyperparameters, we performed a literature review of common Doc2Vec hyperparameters and their different possibilities. Results can be consulted [here](https://github.com/zbmed-semtec/medline-preprocessing/tree/main/resources). We opted to use a dictonary of the hyperparameters since this allows for an easy hyperparameter search implementation. Please, refer to the [tendency analysis](https://github.com/zbmed-semtec/hybrid-dictionary-ner-doc2vec-doc-relevance/blob/main/docs/tendency_analysis/tutorial_tendency_analysis.ipynb) study in which the different hyperparamters are tested.\n",
    "\n",
    "4. To create the model, we just need the input hyperparameters and the tagged data to build a vocabulary. The vocabulary building step is executed in here in order to better separate the model creating from its training, but it could be constructed at the training time without any problem.\n",
    "\n",
    "5. To train the model, we use the number of epochs selected in the model parameters and the examples provided in the tagged data. Additionally, we provide a logging implementation to obtain information about the training:\n",
    "\n",
    "    * Warning/Errors (verbose = 0): default information logged by `gensim` if any error occurs during the training.\n",
    "\n",
    "    * Info (verbose = 1): provides information about the total training time (in seconds).\n",
    "\n",
    "    * Debug (verbose = 2): shows the time at which every epoch starts and finishes.\n",
    "\n",
    "    Lastly, once the model is trained, it can be stored in disk to load later.\n",
    "\n",
    "6. The last step is to generate the embeddings for each publication. This allows to later calculate the cosine similarities for each pair of documents without the need of the `Doc2Vec` model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions\n",
    "\n",
    "* The parameters are passed to the `generate_doc2vec_model()` function as a dictionary to later facilitate the inclusion of hyperparemeter optimization as well as providing an easy to use and implement feaure.\n",
    "\n",
    "* In the training process, we employed the `logging` library to provide information about to the end user. The information reported is selected with the `verbose` parameter as explained in the section before.\n",
    "\n",
    "* **MISSING EMBEDDINGS OUTPUT DECISIONS**\n",
    "\n",
    "* **MISSING VOCABULARY CREATION DECISIONS (if needed)**\n",
    "\n",
    "<! ---\n",
    "The vocabulary is built when the model is created followed the tutorials in their documentation. The vocabulary construction can be executed automatically either at model initiailization and model training, but to provide a clearer pipeline, it is left manua\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "The time to train each dataset (TREC or RELISH) using 8 cores of an Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz with 16GB of RAM running Ubuntu 20.04 LTS and Python 3.8.10 is:\n",
    "\n",
    "* RELISH (163189 publications): 25 seconds per epoch on average.\n",
    "\n",
    "* TREC (32604 publications): 5 seconds per epoch on average.\n",
    "\n",
    "These results will greatly depend on the chosen hyperparameters. The results are for the ones provided in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Include library dependencies in prerequisites.\n",
    "\n",
    "* Finish the decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMOVE THIS LINE BEFORE FINAL VERSION COMMIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook docs/embeddings/tutorial_embeddings.ipynb to markdown\n",
      "[NbConvertApp] Writing 9069 bytes to docs/embeddings/README.md\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert docs/embeddings/tutorial_embeddings.ipynb --to markdown --output README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
